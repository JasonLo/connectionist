{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: simple recurrent neural network with time-averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense layer with Time-averaging mechanism.\n",
      "\n",
      "    In short, time-averaging mechanism simulates continuous-temporal dynamics in a discrete-time recurrent neural networks.\n",
      "    See Plaut, McClelland, Seidenberg, and Patterson (1996) equation (15) for more details.\n",
      "\n",
      "    Args:\n",
      "        tau (float): Time-averaging parameter (How much information should take from the new input). range: [0, 1].\n",
      "\n",
      "        average_at (str): Where to average. Options: 'before_activation', 'after_activation'.\n",
      "\n",
      "            When average_at is 'before_activation', the time-averaging is applied BEFORE activation. i.e., time-averaging INPUT.:\n",
      "                outputs = activation(integrated_input);\n",
      "                integrated input = tau * (inputs @ weights + bias) + (1-tau) * last_inputs;\n",
      "                last_inputs is obtained from the last call of this layer, its values stored at `self.states`\n",
      "\n",
      "            When average_at is 'after_activation', the time-averaging is applied AFTER activation. i.e., time-averaging OUTPUT.:\n",
      "                outputs = tau * activation(inputs @ weights + bias) + (1-tau) * last_outputs;\n",
      "                last_outputs is obtained from the last call of this layer, its values stored at `self.states`\n",
      "\n",
      "        kwargs (str, optional): Any argument in keras.layers.Dense. (e.g., units, activation, use_bias, kernel_initializer, bias_initializer,\n",
      "            kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs).\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from connectionist.layers import TimeAveragedDense\n",
    "\n",
    "print(TimeAveragedDense.__doc__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy model with only one TimeAveragedDense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeAveragedDense(tau=0.2, average_at=\"after_activation\", units=3)\n",
    "x = tf.constant([[1.0, 2.0]])\n",
    "model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in a typical Dense layer, given the same input, the output will be the same regardless of how many time the model is called\n",
    "- but it is not the case in TimeAveragedDense layer, the output will be different each time the model is called\n",
    "- this is the core mechanism of TimeAveragedDense layer, it is a kind of a \"dampening\" layer, more time the layer is called, the output will be closer to the asymptotic value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to build a toy RNN Cell with TimeAveragedDense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "class RNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, tau, units):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.recurrent_dense = tf.keras.layers.Dense(self.units, use_bias=False)\n",
    "        self.input_dense = tf.keras.layers.Dense(self.units, use_bias=False)\n",
    "        self.sum = tf.keras.layers.Add()\n",
    "        self.time_averaged_dense = TimeAveragedDense(tau=self.tau, average_at=\"after_activation\", units=self.units, activation='sigmoid')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states=None):\n",
    "        if states is None:\n",
    "            outputs = self.input_dense(inputs)\n",
    "        else:\n",
    "            outputs = self.sum([self.input_dense(inputs), self.recurrent_dense(states)])\n",
    "        \n",
    "        outputs = self.time_averaged_dense(outputs)\n",
    "        return outputs, outputs\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.time_averaged_dense.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = RNNCell(tau=0.2, units=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually unroll the RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([[1.0, 2.0, 3.0]])\n",
    "states = None\n",
    "ys = []\n",
    "for _ in range(10):\n",
    "    y, state = cell(x, states)\n",
    "    ys.append(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(np.stack(ys).squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unroll in a proper keras layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAveragedRNN(tf.keras.layers.Layer):\n",
    "    def __init__(self, tau, units):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.rnn_cell = RNNCell(tau=self.tau, units=self.units)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_ticks = inputs.shape[1]  # (batch_size, seq_len, input_dim)\n",
    "        outputs = tf.TensorArray(dtype=tf.float32, size=max_ticks)\n",
    "        states = None\n",
    "\n",
    "        for t in range(max_ticks):\n",
    "            this_tick_input = inputs[:, t, :]\n",
    "            states, output = self.rnn_cell(this_tick_input, states=states)\n",
    "            outputs = outputs.write(t, output)\n",
    "\n",
    "        # states persist across tick, but not across batches, so we need to reset it\n",
    "        self.rnn_cell.reset_states()\n",
    "        outputs = outputs.stack()  # (seq_len, batch_size, units)\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])  # (batch_size, seq_len, units)\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small imporvements compared with manually unrolling the RNN Cell:\n",
    "\n",
    "- we infer the number of time ticks from the input shape, it allows time varying inputs\n",
    "- Insteal of a ys list, we use a outputs = tf.TensorArray, it allows long time series and avoid memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = TimeAveragedRNN(tau=0.2, units=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((1, 10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = rnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y.numpy().squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
