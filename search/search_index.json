{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Connectionist's documentation","text":"<p>Connectionist contains some tools for classical connectionist models of reading in TensorFlow. This project is a course companion python library for Contemporary neural networks for cognition and cognitive neuroscience.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Ready-to-use models of reading in TensorFlow</li> <li>Various \"brain\" (model) damaging APIs</li> <li>Basic building blocks (layers) for connectionist models</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python &gt;=3.8</li> <li>TensorFlow &gt;=2.9</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install connectionist\n</code></pre>"},{"location":"#quick-start","title":"Quick start","text":"<p>End-to-end toy example with Plaut, McClelland, Seidenberg and Patterson (1996), simulation 3 model:</p> <pre><code>import tensorflow as tf\nfrom connectionist.data import ToyOP\nfrom connectionist.models import PMSP\n\ndata = ToyOP()\nmodel = PMSP(tau=0.2, h_units=10, p_units=9, c_units=5)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n)\nmodel.fit(data.x_train, data.y_train, epochs=3, batch_size=20)\nmodel(data.x_train)\n</code></pre>"},{"location":"layers/HNSCell/","title":"HNSCell","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>A RNN cell in the hub-and-spokes model. Defines the forward pass of a single time tick.</p> <p>This cell contains one hub layer and more than one spoke layers. The hub layer is a MultiInputTimeAveraging layer, and the spoke layers are HNSSpoke layers.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time-averaging parameter, from 0 to 1.</p> required <code>hub_name</code> <code>str</code> <p>Name of the hub layer.</p> required <code>hub_units</code> <code>int</code> <p>Number of units in the hub layer.</p> required <code>spoke_names</code> <code>List[str]</code> <p>List of names of the spoke layers.</p> required <code>spoke_units</code> <code>List[int]</code> <p>List of number of units in the spoke layers, length must match <code>spoke_names</code>.</p> required"},{"location":"layers/HNSCell/#connectionist.layers.HNSCell.internals_names","title":"<code>internals_names: List[str]</code>  <code>property</code>","text":"<p>Return all the internal connection names.</p>"},{"location":"layers/HNSCell/#connectionist.layers.HNSCell.build","title":"<code>build(input_shape)</code>","text":"<p>Build the cell.</p>"},{"location":"layers/HNSCell/#connectionist.layers.HNSCell.call","title":"<code>call(inputs=None, last_act_hub=None, last_act_spokes=None, return_internals=False)</code>","text":"<p>Forward pass of the HNSCell.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>inputs to each spoke with spoke names as keys.</p> <code>None</code> <code>last_act_hub</code> <code>tf.Tensor</code> <p>last activation of the hub.</p> <code>None</code> <code>last_act_spokes</code> <code>dict</code> <p>last activations of the spokes with spoke names as keys.</p> <code>None</code> <code>return_internals</code> <code>bool</code> <p>Whether to return intermediate inputs to each connection.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>outputs</code> <code>Dict[str, tf.Tensor]</code> <p>Activations in each layer <code>{\"hub_name\": ..., \"spoke_name\": }</code>, if <code>return_internals=True</code>, also return intermediate inputs in each connection. E.g., <code>{\"hub_name2spoke_name\": last_act_hub @ w_{hs_i}, ...}</code></p>"},{"location":"layers/HNSCell/#connectionist.layers.HNSCell.reset_states","title":"<code>reset_states()</code>","text":"<p>Reset the time-averaging states in all hub and spokes.</p>"},{"location":"layers/HNSLayer/","title":"HNSLayer","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Hub-and-Spokes RNN Layer.</p> <p>Unrolling the HNSCell for a fixed number of time steps.</p> <p>See Rogers et. al., 2004 for more details.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time constant of the time-averaging.</p> required <code>hub_name</code> <code>str</code> <p>Name of the hub layer.</p> required <code>hub_units</code> <code>int</code> <p>Number of units in the hub layer.</p> required <code>spoke_names</code> <code>List[str]</code> <p>Names of the spoke layers.</p> required <code>spoke_units</code> <code>List[int]</code> <p>Number of units in each spoke layer. Must be the same length as <code>spoke_names</code>.</p> required"},{"location":"layers/HNSLayer/#connectionist.layers.HNSLayer.build","title":"<code>build(input_shape)</code>","text":"<p>Build the HNSCell.</p>"},{"location":"layers/HNSLayer/#connectionist.layers.HNSLayer.call","title":"<code>call(inputs=None, return_internals=False)</code>","text":"<p>Forward pass: unroll the HNSCell for a fixed number of time steps.</p> <p>The number of time steps is determined by axis 1 in the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, tf.Tensor]</code> <p>Inputs to the spokes (name as key). Assumes input is 0 if not given.</p> <code>None</code> <code>return_internals</code> <code>bool</code> <p>Whether to return intermediate inputs to each connection.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, tf.Tensor]</code> <p>Dict[str, tf.Tensor]: Activations of the hub and spokes, with layer names as keys. If <code>return_internals</code> is True, also returns intermediate inputs to each connection.</p>"},{"location":"layers/HNSSpoke/","title":"HNSSpoke","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>A spoke in the hub-and-spokes model with time-averaging output.</p> <p>This layer is a thin wrapper around MultiInputTimeAveraging, The differences are:</p> <ol> <li>Catering for <code>None</code> input by explicitly defining <code>units</code>.</li> <li>For clarity, in <code>call()</code>, separating the original list of inputs into the clamped input: <code>inputs</code> and last states: <code>last_states</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time-averaging parameter, from 0 to 1.</p> required <code>units</code> <code>int</code> <p>Number of units in the layer. Explicitly set the input dimension to cater for <code>None</code> input.</p> required <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>'sigmoid'</code> <code>use_bias</code> <code>bool</code> <p>Whether the layer uses a bias vector.</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to MultiInputTimeAveraging.</p> <code>{}</code> <p>Forward pass:</p> <p>at=\u03c4\u22c5act(xt+\u2211insi,t\u22121+b)+(1\u2212\u03c4)\u22c5at\u22121 a_t = \\tau \\cdot act(x_t + \\sum_i^n s_{i, t-1} + b) + (1-\\tau) \\cdot a_{t-1} at\u200b=\u03c4\u22c5act(xt\u200b+i\u2211n\u200bsi,t\u22121\u200b+b)+(1\u2212\u03c4)\u22c5at\u22121\u200b</p> <p>Warning</p> <p>\"\u22c5\\cdot\u22c5\" is element-wise multiplication.</p> <ul> <li>ata_tat\u200b: activation at time ttt.</li> <li>\u03c4\\tau\u03c4: time constant, smaller means slower temporal dynamics.</li> <li>actactact: activation function (provided by this layer if <code>activation</code> is used).</li> <li>xtx_{t}xt\u200b: input at time ttt coming from input iii.</li> <li>si,t\u22121s_{i, t-1}si,t\u22121\u200b: last state from iii at time t\u22121t-1t\u22121.</li> <li>nnn: number of cross-tick state layers.</li> <li>bbb: bias vector, provided by this layer if <code>use_bias</code> is <code>True</code> (Default).</li> <li>at\u22121a_{t-1}at\u22121\u200b: time-averaging state at time t\u22121t-1t\u22121, stored in <code>self.time_averaging.states</code>.</li> </ul>"},{"location":"layers/HNSSpoke/#connectionist.layers.HNSSpoke.call","title":"<code>call(inputs=None, last_states=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code> <p>clamped inputs to spoke.</p> <code>None</code> <code>last_states</code> <code>List[tf.Tensor]</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>list of states from last ticks projection: si,t\u22121s_{i, t-1}si,t\u22121\u200b.</p> <code>None</code>"},{"location":"layers/HNSSpoke/#connectionist.layers.HNSSpoke.reset_states","title":"<code>reset_states()</code>","text":"<p>Reset time-averaging states.</p>"},{"location":"layers/MultiInputTimeAveraging/","title":"MultiInputTimeAveraging","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>This layer simulates continuous-temporal dynamics with time-averaged input/output for multiple inputs.</p> <p>Note</p> <p>This layer is for multiple inputs, and assuming they had ALREADY been multiplied by weights. i.e., This layer has no weights matrix, but a bias vector if <code>use_bias</code> is <code>True</code>. For single input equivalent, see TimeAveragedDense.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time-averaging parameter, from 0 to 1.</p> required <code>average_at</code> <code>str</code> <p>Select where to average, 'before_activation' or 'after_activation'.</p> required <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether the layer uses a bias vector.</p> <code>True</code> <code>bias_initializer</code> <code>optional</code> <p>Initializer for the bias vector.</p> <code>'zeros'</code> <code>bias_regularizer</code> <code>optional</code> <p>Regularizer function applied to the bias vector.</p> <code>None</code> <code>bias_constraint</code> <code>optional</code> <p>Constraint function applied to the bias vector.</p> <code>None</code>"},{"location":"layers/MultiInputTimeAveraging/#connectionist.layers.MultiInputTimeAveraging--time-averaged-input","title":"Time-averaged input","text":"<p>See Plaut, McClelland, Seidenberg, and Patterson (1996) equation (15).</p> <p>Defines as:</p> <p><p>at=act(st) a_t = act(s_t) at\u200b=act(st\u200b)</p> <p>st=\u03c4\u22c5(\u2211inxi,t+b)+(1\u2212\u03c4)\u22c5st\u22121 s_t = \\tau \\cdot (\\sum_i^n x_{i,t} + b) + (1-\\tau) \\cdot s_{t-1} st\u200b=\u03c4\u22c5(i\u2211n\u200bxi,t\u200b+b)+(1\u2212\u03c4)\u22c5st\u22121\u200b</p></p> <p>Warning</p> <p>\"\u22c5\\cdot\u22c5\" is element-wise multiplication.</p> <ul> <li>ata_tat\u200b: activation at time ttt.</li> <li>actactact: activation function (provided by this layer if <code>activation</code> is used).</li> <li>sts_tst\u200b: state at time ttt.</li> <li>\u03c4\\tau\u03c4: time constant, smaller means slower temporal dynamics.</li> <li>xi,tx_{i,t}xi,t\u200b: input at time ttt coming from input iii.</li> <li>nnn: number of input layers.</li> <li>bbb: bias vector, provided by this layer if <code>use_bias</code> is <code>True</code> (Default).</li> <li>st\u22121s_{t-1}st\u22121\u200b: state at time t\u22121t-1t\u22121, stored in <code>self.states</code>.</li> </ul> <p>Example</p> <pre><code>layer = MultiInputTimeAveraging(\n    tau=0.1, average_at=\"before_activation\", activation=\"sigmoid\"\n    )\n</code></pre>"},{"location":"layers/MultiInputTimeAveraging/#connectionist.layers.MultiInputTimeAveraging--time-averaged-output","title":"Time-averaged output","text":"<p>Defines as:</p> <p>at=\u03c4\u22c5act(\u2211inxi,t+b)+(1\u2212\u03c4)\u22c5at\u22121 a_t = \\tau \\cdot act(\\sum_i^n x_{i,t} + b) + (1-\\tau) \\cdot a_{t-1} at\u200b=\u03c4\u22c5act(i\u2211n\u200bxi,t\u200b+b)+(1\u2212\u03c4)\u22c5at\u22121\u200b</p> <p>Warning</p> <p>\"\u22c5\\cdot\u22c5\" is element-wise multiplication.</p> <ul> <li>ata_tat\u200b: activation at time ttt</li> <li>\u03c4\\tau\u03c4: time constant, smaller means slower temporal dynamics</li> <li>actactact: activation function (provided by this layer if <code>activation</code> is used)</li> <li>xi,tx_{i,t}xi,t\u200b: input at time ttt coming from input iii</li> <li>bbb: bias vector, provided by this layer if <code>use_bias</code> is <code>True</code> (Default)</li> <li>at\u22121a_{t-1}at\u22121\u200b: activation at time t\u22121t-1t\u22121, stored in <code>self.states</code></li> </ul> <p>Example</p> <pre><code>layer = MultiInputTimeAveraging(\n    tau=0.1, average_at=\"after_activation\", activation=\"sigmoid\"\n    )\n</code></pre>"},{"location":"layers/MultiInputTimeAveraging/#connectionist.layers.MultiInputTimeAveraging.build","title":"<code>build(input_shape)</code>","text":"<p>Build the underlying layers/weights.</p>"},{"location":"layers/MultiInputTimeAveraging/#connectionist.layers.MultiInputTimeAveraging.call","title":"<code>call(inputs)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[tf.Tensor]</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>List of input tensors xi,tx_{i,t}xi,t\u200b.</p> required <p>Returns:</p> Name Type Description <code>outputs</code> <code>tf.Tensor</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Output tensor ata_tat\u200b.</p>"},{"location":"layers/MultiInputTimeAveraging/#connectionist.layers.MultiInputTimeAveraging.reset_states","title":"<code>reset_states()</code>","text":"<p>Resetting <code>self.states</code> to None.</p> <p>Note</p> <p>This is typically called when RNN unrolling is done, so that the next batch of data can start with a clean state.</p>"},{"location":"layers/PMSPCell/","title":"PMSPCell","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>RNN cell for PMSP model.</p> <p>See Plaut, McClelland, Seidenberg and Patterson (1996), simulation 3.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time-averaging parameter, from 0 to 1.</p> required <code>h_units</code> <code>int</code> <p>Number of units in the hidden layer.</p> required <code>p_units</code> <code>int</code> <p>Number of units in the phonological layer.</p> required <code>c_units</code> <code>int</code> <p>Number of units in the cleanup layer.</p> required <code>h_noise</code> <code>float</code> <p>Gaussian noise parameter (in stddev) for hidden layer.</p> <code>0.0</code> <code>p_noise</code> <code>float</code> <p>Gaussian noise parameter (in stddev) for phonological layer.</p> <code>0.0</code> <code>c_noise</code> <code>float</code> <p>Gaussian noise parameter (in stddev) for cleanup layer.</p> <code>0.0</code> <code>connections</code> <code>List[str]</code> <p>List of connections to use, each connection consists of two letters (from, to). Default is [\"oh\", \"ph\", \"hp\", \"pp\", \"cp\", \"pc\"].</p> <code>None</code> <code>zero_out_rates</code> <code>Dict[str, float]</code> <p>Dictionary of zero-out rates for each connection. Default is <code>{c: 0.0 for c in self.connections}</code>. See PMSP for more details.</p> <code>None</code> <code>l2</code> <code>float</code> <p>L2 regularization parameter, apply to all trainable weights and biases.</p> <code>0.0</code>"},{"location":"layers/PMSPCell/#connectionist.layers.PMSPCell.all_layers_names","title":"<code>all_layers_names: List[str]</code>  <code>property</code>","text":"<p>List of all layer full names.</p>"},{"location":"layers/PMSPCell/#connectionist.layers.PMSPCell.build","title":"<code>build(input_shape)</code>","text":"<p>Build the layer.</p>"},{"location":"layers/PMSPCell/#connectionist.layers.PMSPCell.call","title":"<code>call(last_o, last_h, last_p, last_c, training=False, return_internals=False)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>last_o</code> <code>tf.Tensor</code> <p>Orthographic inputs from previous timestep, shape: (batch_size, input_units).</p> required <code>last_h</code> <code>tf.Tensor</code> <p>Hidden layer activations from previous timestep, shape: (batch_size, h_units).</p> required <code>last_p</code> <code>tf.Tensor</code> <p>Phonology layer activations from previous timestep, shape: (batch_size, p_units).</p> required <code>last_c</code> <code>tf.Tensor</code> <p>Cleanup layer activations from previous timestep, shape: (batch_size, c_units).</p> required <code>training</code> <code>bool</code> <p>Whether in training mode.</p> <code>False</code> <code>return_internals</code> <code>bool</code> <p>Whether to return intermediate inputs to each connection.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>outputs</code> <code>Dict[str, tf.Tensor]</code> <p>Activations in each layer <code>{\"phonology\": ...}</code>, if <code>return_internals=True</code>, also return intermediate inputs in each connection. E.g., <code>{\"oh\": last_o @ w_{oh}, ...}</code></p>"},{"location":"layers/PMSPCell/#connectionist.layers.PMSPCell.get_connection_units","title":"<code>get_connection_units(connection)</code>","text":"<p>Get number of output units for a given connection.</p> <p>Parameters:</p> Name Type Description Default <code>connection</code> <code>str</code> <p>Connection string, select from <code>self.connections</code>.</p> required <p>Returns:</p> Name Type Description <code>units</code> <code>int</code> <p>Number of output units.</p>"},{"location":"layers/PMSPCell/#connectionist.layers.PMSPCell.get_connections","title":"<code>get_connections(layer)</code>","text":"<p>Get connections that end with a given layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>Layer name, select from <code>self.all_layers_names</code>.</p> required <p>Returns:</p> Name Type Description <code>connections</code> <code>List[str]</code> <p>List of connections that end with the given layer.</p> <p>Example</p> <p>if <code>layer = \"hidden\"</code>, return all connections that ends with \"h\", e.g.: <code>[\"oh\", \"ph\"]</code></p> <pre><code>cell.get_connections(\"hidden\")\n&gt;&gt;&gt; [\"oh\", \"ph\"]\n</code></pre>"},{"location":"layers/PMSPCell/#connectionist.layers.PMSPCell.reset_states","title":"<code>reset_states()</code>","text":"<p>Reset all time-averaging states.</p>"},{"location":"layers/PMSPCell/#connectionist.layers.PMSPCell.zero_out_weights","title":"<code>zero_out_weights()</code>","text":"<p>Assign zero values to weights by its masks in all connections.</p> <p>See ZeroOutDense for more details.</p>"},{"location":"layers/PMSPLayer/","title":"PMSPLayer","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>PMSP sim 3 model RNN layer.</p> <p>Unrolling the PMSPCell for a fixed number of time steps.</p> <p>See Plaut, McClelland, Seidenberg and Patterson (1996), simulation 3.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time-averaging parameter, from 0 to 1.</p> required <code>h_units</code> <code>int</code> <p>Number of units in the hidden layer.</p> required <code>p_units</code> <code>int</code> <p>Number of units in the phonological layer.</p> required <code>c_units</code> <code>int</code> <p>Number of units in the cleanup layer.</p> required <code>h_noise</code> <code>float</code> <p>Gaussian noise parameter (in stddev) for hidden layer.</p> <code>0.0</code> <code>p_noise</code> <code>float</code> <p>Gaussian noise parameter (in stddev) for phonological layer.</p> <code>0.0</code> <code>c_noise</code> <code>float</code> <p>Gaussian noise parameter (in stddev) for cleanup layer.</p> <code>0.0</code> <code>connections</code> <code>List[str]</code> <p>List of connections to use, each connection consists of two letters (from, to). Default is [\"oh\", \"ph\", \"hp\", \"pp\", \"cp\", \"pc\"].</p> <code>None</code> <code>zero_out_rates</code> <code>Dict[str, float]</code> <p>Dictionary of zero-out rates for each connection. Default is <code>{c: 0.0 for c in self.connections}</code>. See PMSP for more details.</p> <code>None</code> <code>l2</code> <code>float</code> <p>L2 regularization parameter, apply to all trainable weights and biases.</p> <code>0.0</code>"},{"location":"layers/PMSPLayer/#connectionist.layers.PMSPLayer.build","title":"<code>build(input_shape)</code>","text":"<p>Build the layers.</p>"},{"location":"layers/PMSPLayer/#connectionist.layers.PMSPLayer.call","title":"<code>call(inputs, training=False, return_internals=False)</code>","text":"<p>Forward pass, unrolling the RNN cell.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code> <p>Input tensor with shape (batch_size, max_ticks, input_units).</p> required <code>training</code> <code>bool</code> <p>Whether to run in training mode or not.</p> <code>False</code> <code>return_internals</code> <code>bool</code> <p>Whether to return intermediate inputs to each connection.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>outputs</code> <code>Dict[str, tf.Tensor]</code> <p>Activations with shape (batch_size, max_ticks, units) in each layer. E.g.: <code>{\"phonology\": ...}</code>, if <code>return_internals=True</code>, also return intermediate inputs with shape (batch_size, max_ticks, units) in each connection. E.g., <code>{\"oh\": last_o @ w_{oh}, ...}</code></p>"},{"location":"layers/TimeAveragedDense/","title":"TimeAveragedDense","text":"<p>         Bases: <code>tf.keras.layers.Dense</code></p> <p>This layer simulates continuous-temporal dynamics with time-averaged input/output.</p> <p>Note</p> <p>This layer is for single input, i.e., signal comes from one precedent layer. For multiple inputs equivalent, see MultiInputTimeAveraging.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time-averaging parameter, from 0 to 1.</p> required <code>average_at</code> <code>str</code> <p>Select where to average, 'before_activation' or 'after_activation'.</p> required <code>kwargs</code> <code>dict</code> <p>Any argument in <code>keras.layers.Dense</code>.</p> <code>{}</code>"},{"location":"layers/TimeAveragedDense/#connectionist.layers.TimeAveragedDense--time-averaged-input","title":"Time-averaged input","text":"<p>See Plaut, McClelland, Seidenberg, and Patterson (1996) equation (15).</p> <p>Defines as:</p> <p><p>at=act(st) a_t = act(s_t) at\u200b=act(st\u200b)</p> <p>st=\u03c4\u22c5(xtw+b)+(1\u2212\u03c4)\u22c5st\u22121 s_t = \\tau \\cdot (x_t w + b) + (1-\\tau) \\cdot s_{t-1} st\u200b=\u03c4\u22c5(xt\u200bw+b)+(1\u2212\u03c4)\u22c5st\u22121\u200b</p></p> <p>Warning</p> <p>\"\u22c5\\cdot\u22c5\" is element-wise multiplication.</p> <ul> <li>ata_tat\u200b: activation at time ttt.</li> <li>actactact: activation function (provided by this layer if <code>activation</code> is used).</li> <li>sts_tst\u200b: state at time ttt.</li> <li>\u03c4\\tau\u03c4: time constant, smaller means slower temporal dynamics.</li> <li>xtx_txt\u200b: input at time ttt.</li> <li>www: weight matrix (provided by this layer).</li> <li>bbb: bias vector, provided by this layer if <code>use_bias</code> is <code>True</code> (Default).</li> <li>st\u22121s_{t-1}st\u22121\u200b: state at time t\u22121t-1t\u22121, stored in <code>self.states</code>.</li> </ul> <p>Example</p> <pre><code>layer = TimeAveragedDense(\n    tau=0.1, average_at=\"before_activation\", units=10, activation=\"sigmoid\"\n    )\n</code></pre>"},{"location":"layers/TimeAveragedDense/#connectionist.layers.TimeAveragedDense--time-averaged-output","title":"Time-averaged output","text":"<p>Defines as:</p> <p>at=\u03c4\u22c5act(xtw+b)+(1\u2212\u03c4)\u22c5at\u22121 a_t = \\tau \\cdot act(x_t w + b) + (1-\\tau) \\cdot a_{t-1} at\u200b=\u03c4\u22c5act(xt\u200bw+b)+(1\u2212\u03c4)\u22c5at\u22121\u200b</p> <p>Warning</p> <p>\"\u22c5\\cdot\u22c5\" is element-wise multiplication.</p> <ul> <li>ata_tat\u200b: activation at time ttt.</li> <li>\u03c4\\tau\u03c4: time constant, smaller means slower temporal dynamics.</li> <li>actactact: activation function (provided by this layer if <code>activation</code> is used).</li> <li>xtx_txt\u200b: input at time ttt.</li> <li>www: weight matrix (provided by this layer).</li> <li>bbb: bias vector, provided by this layer if <code>use_bias</code> is <code>True</code> (Default).</li> <li>at\u22121a_{t-1}at\u22121\u200b: activation at time t\u22121t-1t\u22121, stored in <code>self.states</code>.</li> </ul> <p>Example</p> <pre><code>layer = TimeAveragedDense(\n    tau=0.1, average_at=\"after_activation\", units=10, activation=\"sigmoid\"\n    )\n</code></pre>"},{"location":"layers/TimeAveragedDense/#connectionist.layers.TimeAveragedDense.call","title":"<code>call(inputs)</code>","text":"<p>Forward pass.</p> <p>Note</p> <p>This function reused <code>keras.layers.Dense.call()</code> without activation. Time-averaging and activation are manually implemented here.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Input tensor xtx_txt\u200b.</p> required <p>Returns:</p> Name Type Description <code>outputs</code> <code>tf.Tensor</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Output tensor ata_tat\u200b.</p>"},{"location":"layers/TimeAveragedDense/#connectionist.layers.TimeAveragedDense.reset_states","title":"<code>reset_states()</code>","text":"<p>Resetting <code>self.states</code> to None.</p> <p>Note</p> <p>This is typically called when RNN unrolling is done, so that the next batch of data can start with a clean state.</p>"},{"location":"layers/TimeAveragedRNN/","title":"TimeAveragedRNN","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>A simple RNN with time-averaging output.</p> <p>Note</p> <p>This layer unrolls TimeAveragedRNNCell for a fixed number of steps. The number of steps is determined by \"sequence length\" in axis 1 of input tensor's shape.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time-averaging parameter, from 0 to 1.</p> required <code>units</code> <code>int</code> <p>Number of units in the RNN cell.</p> required"},{"location":"layers/TimeAveragedRNN/#connectionist.layers.TimeAveragedRNN.build","title":"<code>build(input_shape)</code>","text":"<p>Build the underlying layers/weights.</p>"},{"location":"layers/TimeAveragedRNN/#connectionist.layers.TimeAveragedRNN.call","title":"<code>call(inputs)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code> <p>Input tensor xtx_txt\u200b with shape (batch_size, seq_len, input_dim).</p> required <p>Returns:</p> Name Type Description <code>outputs</code> <code>tf.Tensor</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Output tensor hth_tht\u200b with shape (batch_size, seq_len, units).</p>"},{"location":"layers/TimeAveragedRNNCell/","title":"TimeAveragedRNNCell","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>A simple RNN cell with time-averaging output. It defines one step of compute in an RNN.</p> <p>Note</p> <p>This layer serves as an example of how to use <code>MultiInputTimeAveraging</code> in a custom RNN cell.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time-averaging parameter, from 0 to 1.</p> required <code>units</code> <code>int</code> <p>Number of units in the RNN cell.</p> required"},{"location":"layers/TimeAveragedRNNCell/#connectionist.layers.TimeAveragedRNNCell--forward-pass","title":"Forward pass","text":"<p>ht=\u03c4\u22c5\u03c3(ht\u22121whh+xtwxh+b)+(1\u2212\u03c4)\u22c5ht\u22121 h_t = \\tau \\cdot \\sigma(h_{t-1} w_{hh} + x_t w_{xh} + b) + (1 - \\tau) \\cdot h_{t-1} ht\u200b=\u03c4\u22c5\u03c3(ht\u22121\u200bwhh\u200b+xt\u200bwxh\u200b+b)+(1\u2212\u03c4)\u22c5ht\u22121\u200b</p> <p>Warning</p> <p>\"\u22c5\\cdot\u22c5\" is element-wise multiplication.</p> <ul> <li>hth_tht\u200b is the output (also equal to hidden state) at time ttt.</li> <li>xtx_txt\u200b is the input at time ttt.</li> <li>\u03c3\\sigma\u03c3 is the sigmoid activation function.</li> <li>wxhw_{xh}wxh\u200b is the weight matrix from input to hidden state.</li> <li>whhw_{hh}whh\u200b is the weight matrix from hidden state to hidden state.</li> <li>bbb is the bias vector.</li> <li>\u03c4\\tau\u03c4: time constant, smaller means slower temporal dynamics.</li> </ul>"},{"location":"layers/TimeAveragedRNNCell/#connectionist.layers.TimeAveragedRNNCell.build","title":"<code>build(input_shape)</code>","text":"<p>Build the underlying layers/weights.</p>"},{"location":"layers/TimeAveragedRNNCell/#connectionist.layers.TimeAveragedRNNCell.call","title":"<code>call(inputs, states=None)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Input tensor xtx_txt\u200b with shape (batch_size, input_dim).</p> required <code>states</code> <code>tf.Tensor</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Hidden state tensor ht\u22121h_{t-1}ht\u22121\u200b with shape (batch_size, units).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>states</code> <code>tf.Tensor</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Hidden state tensor hth_tht\u200b.</p> <code>outputs</code> <code>tf.Tensor</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Output tensor hth_tht\u200b. This is the same as the states.</p>"},{"location":"layers/TimeAveragedRNNCell/#connectionist.layers.TimeAveragedRNNCell.reset_states","title":"<code>reset_states()</code>","text":".katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Resetting the time-averaging state to None</p> <p>Note</p> <p>This time-averaging state ht\u22121h_{t-1}ht\u22121\u200b is handled inside the <code>MultiInputTimeAveraging</code> layer, which taking care of the 2nd part of the forward pass: (1\u2212\u03c4)\u22c5ht\u22121(1 - \\tau) \\cdot h_{t-1}(1\u2212\u03c4)\u22c5ht\u22121\u200b.</p> <p>Even though the first part of the forward pass: \u03c4\u22c5\u03c3(ht\u22121whh+xtwxh+b)\\tau \\cdot \\sigma(h_{t-1} w_{hh} + x_t w_{xh} + b)\u03c4\u22c5\u03c3(ht\u22121\u200bwhh\u200b+xt\u200bwxh\u200b+b) is using the same ht\u22121h_{t-1}ht\u22121\u200b values, it is handled explicitly in the <code>call</code> method, so we don't need to reset it here.</p>"},{"location":"layers/ZeroOutDense/","title":"ZeroOutDense","text":"<p>         Bases: <code>tf.keras.layers.Dense</code></p> <p>Dense layer with zero-out (weight masking) mechanism.</p> <p>This masking mechanism includes:</p> <ol> <li>Create an non-trainable persistent mask shapes like the weight matrix.</li> <li>During <code>build</code>, assign 0 values to weight matrix.</li> <li>During forward pass <code>call</code>, element-wise multiply the weight matrix with the mask matrix     to block the gradient from back-propagating to the weight matrix.     Hence, the masked weights are technical not trainable.</li> </ol> <p>Note</p> <p>Why (3) blocks the gradient in the masked weight? Consider the forward-pass of masked=w\u22c5mmasked = w \\cdot mmasked=w\u22c5m, its backward pass is dw=m\u22c5dmaskedd_w = m \\cdot d_{masked}dw\u200b=m\u22c5dmasked\u200b, therefore if mmm is 0, dwd_wdw\u200b is 0, hence the gradient of weight is 0 when its mask's value is 0.</p> <p>Parameters:</p> Name Type Description Default <code>zero_out_rate</code> <code>float</code> <p>Probability of a weight begin masked.</p> required <code>units</code> <code>int</code> <p>Number of units in the layer.</p> required <code>activation</code> <code>str</code> <p>Activation function to use.</p> <code>None</code> <code>use_bias</code> <code>bool</code> <p>Whether the layer uses a bias vector.</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Any Keyword arguments in <code>tf.keras.layers.Dense</code>.</p> <code>{}</code>      .katex img {       object-fit: fill;       padding: unset;       display: block;       position: absolute;       width: 100%;       height: inherit;     }  <p>Forward pass: <p>y=x(w\u22c5m)+b y = x(w \\cdot m) + b y=x(w\u22c5m)+b</p></p> <p>Warning</p> <p>\"\u22c5\\cdot\u22c5\" is element-wise multiplication.</p> <ul> <li>xxx: input tensor.</li> <li>www: weight matrix provided by this layer, trainable.</li> <li>mmm: mask matrix, provided by this layer, NOT trainable.</li> <li>bbb: bias vector, provided by this layer if <code>use_bias</code> is <code>True</code> (Default), trainable.</li> </ul>"},{"location":"layers/ZeroOutDense/#connectionist.layers.ZeroOutDense.build","title":"<code>build(input_shape)</code>","text":"<p>Build the layer.</p>"},{"location":"layers/ZeroOutDense/#connectionist.layers.ZeroOutDense.call","title":"<code>call(inputs)</code>","text":"<p>Forward pass.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code> <p>Input tensor.</p> required <p>Returns:</p> Name Type Description <code>outputs</code> <code>tf.Tensor</code> <p>Output tensor.</p>"},{"location":"layers/ZeroOutDense/#connectionist.layers.ZeroOutDense.zero_out_weights","title":"<code>zero_out_weights()</code>","text":"<p>Manually assign zero values to the all weights using the mask.</p>"},{"location":"layers/overview/","title":"connectionist.layers","text":""},{"location":"layers/overview/#core-layers","title":"Core layers","text":"<p>One of the core mechanism in connectionist models is the time-averaging input/output. Similar to Forward Euler method, the time-averaging input/output is a discrete approximation of the continuous time dynamics. There are 2 layers in this module that implements this mechanism:</p> <ul> <li>TimeAveragedDense layer: Simulating continuous time with discrete approximation for a single layer input.</li> <li>MultiInputTimeAveraging layer: Simulating continuous time with discrete approximation for multiple layer inputs.</li> </ul>"},{"location":"layers/overview/#rnn-building-blocks","title":"RNN building blocks","text":"<p>All the RNNs in this module has the time-averaging mechanism.</p>"},{"location":"layers/overview/#simple-rnn","title":"Simple RNN","text":"<p>Mainly for the purpose of demonstration, there are 2 simple RNN layers in this module:</p> <ul> <li>TimeAveragedRNNCell layer: Defines one step of compute.</li> <li>TimeAveragedRNN layer: Unrolling the <code>TimeAveragedRNNCell</code> for multiple steps, similar to <code>tf.keras.layers.SimpleRNN</code>, but with time-averaging output mechanism.</li> </ul>"},{"location":"layers/overview/#pmsp","title":"PMSP","text":"<p>Building blocks for PMPS model:</p> <ul> <li>ZeroOutDense layer: A wrapper layer for <code>tf.keras.layers.Dense</code> that zero out a portion of the weights in the layer, mainly for model.zero_out \"brain\" damage API.</li> <li>PMSPCell layer: Defines one step of compute.</li> <li>PMSPLayer layer: Unrolling the <code>PMSPCell</code> for multiple steps, describe the entire model architecture of PMSP.</li> </ul>"},{"location":"layers/overview/#hub-and-spokes","title":"Hub-and-spokes","text":"<p>Building blocks for Hub-and-spokes model:</p> <ul> <li>HNSSpoke layer: Defines one spoke.</li> <li>HNSCell layer: Defines one step of compute.</li> <li>HNSLayer layer: Unrolling the <code>HNSCell</code> for multiple steps, describe the entire model architecture of HubAndSpokes.</li> </ul>"},{"location":"losses/MaskedBinaryCrossEntropy/","title":"MaskedBinaryCrossEntropy","text":"<p>         Bases: <code>tf.keras.losses.Loss</code></p> <p>Compute Binary Cross-Entropy with masking.</p> <p>Parameters:</p> Name Type Description Default <code>mask_value</code> <code>int</code> <p>value in y_true to be masked.</p> <code>None</code> <code>name</code> <code>str</code> <p>name of the loss function.</p> <code>'masked_binary_crossentropy'</code> <code>reduction</code> <code>str</code> <p>reduction method for the loss.</p> <code>'none'</code>"},{"location":"losses/MaskedBinaryCrossEntropy/#connectionist.losses.MaskedBinaryCrossEntropy.call","title":"<code>call(y_true, y_pred)</code>","text":"<p>Calculate masked binary cross-entropy.</p> <p>Define as: <p>Hp(q)=\u22121M\u2211i=1Nmi\u22c5(yi\u22c5log\u2061(p(yi))+(1\u2212yi)\u22c5log\u2061(1\u2212p(yi))) H_p(q) = - \\frac{1}{M} \\sum_{i=1}^{N} m_i \\cdot (y_i \\cdot \\log(p(y_i)) + (1-y_i) \\cdot \\log(1-p(y_i))) Hp\u200b(q)=\u2212M1\u200bi=1\u2211N\u200bmi\u200b\u22c5(yi\u200b\u22c5log(p(yi\u200b))+(1\u2212yi\u200b)\u22c5log(1\u2212p(yi\u200b)))</p> where yiy_iyi\u200b is the target, and p(yi)p(y_i)p(yi\u200b) is the prediction mim_imi\u200b is the mask, and MMM is the number of unmasked units.</p> <p>Warning</p> <p>\"\u22c5\\cdot\u22c5\" is element-wise multiplication.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>tf.Tensor</code> <p>target y with shape (batch_size, seq_len, feature)</p> required <code>y_pred</code> <code>tf.Tensor</code> <p>predicted y with shape (batch_size, seq_len, feature)</p> required <p>Returns:</p> Name Type Description <code>Loss</code> <code>tf.Tensor</code> <p>Loss values with shape (batch_size)</p>"},{"location":"models/HubAndSpokes/","title":"HubAndSpokes","text":"<p>         Bases: <code>tf.keras.Model</code></p>"},{"location":"models/HubAndSpokes/#connectionist.models.HubAndSpokes.__init__","title":"<code>__init__(tau, hub_name, hub_units, spoke_names, spoke_units)</code>","text":"<p>Hub-and-spokes model.</p> <p>See Rogers et. al., 2004 for more details.</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time constant of the time-averaging.</p> required <code>hub_name</code> <code>str</code> <p>Name of the hub layer.</p> required <code>hub_units</code> <code>int</code> <p>Number of units in the hub layer.</p> required <code>spoke_names</code> <code>List[str]</code> <p>Names of the spoke layers.</p> required <code>spoke_units</code> <code>List[int]</code> <p>Number of units in each spoke layer. Must be the same length as <code>spoke_names</code>.</p> required"},{"location":"models/HubAndSpokes/#connectionist.models.HubAndSpokes.call","title":"<code>call(inputs, return_internals=False)</code>","text":"<p>Forward pass.</p> <p>The number of time steps is determined by axis 1 in the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Dict[str, tf.Tensor]</code> <p>Inputs to the spokes (name as key). Assumes input is 0 if not given.</p> required <code>return_internals</code> <code>bool</code> <p>Whether to return intermediate inputs to each connection.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, tf.Tensor]</code> <p>Dict[str, tf.Tensor]: Activations of the hub and spokes, with layer names as keys. If <code>return_internals</code> is True, also returns intermediate inputs to each connection.</p>"},{"location":"models/HubAndSpokes/#connectionist.models.HubAndSpokes.train_step","title":"<code>train_step(data)</code>","text":"<p>Train the model for one step.</p> <p>Loss is cumulated over all y_train items.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tuple[Dict[str, tf.Tensor]]</code> <p>Tuple of (x_train, y_train).</p> required <p>Returns:</p> Type Description <code>Dict[str, tf.Tensor]</code> <p>Dict[str, tf.Tensor]: metrics.</p>"},{"location":"models/PMSP/","title":"PMSP","text":"<p>         Bases: <code>tf.keras.Model</code></p> <p>PMSP sim 3 model. A recurrent neural network with time-averaging input that contains 3 inter-connected layers: hidden, phonology, and cleanup.</p> <p>See Plaut, McClelland, Seidenberg and Patterson (1996), simulation 3.</p> <p>This model provides extra functionality for \"brain damage\" experiments, including:</p> <ul> <li><code>shrink_layer</code>: Reduce the number of units in a layer.</li> <li><code>zero_out</code>: Write zero values to a portion of units in a weight matrix and make those units not trainable.</li> <li><code>cut_connections</code>: Remove the specified connections.</li> <li><code>add_noise</code>: Add Gaussian noise to a layer.</li> <li><code>apply_l2</code>: Apply L2 regularization to all trainable weights and biases.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>float</code> <p>Time-averaging parameter, from 0 to 1.</p> required <code>h_units</code> <code>int</code> <p>Number of units in the hidden layer.</p> required <code>p_units</code> <code>int</code> <p>Number of units in the phonological layer.</p> required <code>c_units</code> <code>int</code> <p>Number of units in the cleanup layer.</p> required <code>h_noise</code> <code>float</code> <p>Gaussian noise parameter (in stddev) for hidden layer.</p> <code>0.0</code> <code>p_noise</code> <code>float</code> <p>Gaussian noise parameter (in stddev) for phonological layer.</p> <code>0.0</code> <code>c_noise</code> <code>float</code> <p>Gaussian noise parameter (in stddev) for cleanup layer.</p> <code>0.0</code> <code>connections</code> <code>List[str]</code> <p>List of connections to use, each connection consists of two letters (from, to). Default is [\"oh\", \"ph\", \"hp\", \"pp\", \"cp\", \"pc\"].</p> <code>None</code> <code>zero_out_rates</code> <code>Dict[str, float]</code> <p>Dictionary of zero-out rates for each connection. Default is <code>{c: 0.0 for c in self.connections}</code>. See PMSP for more details.</p> <code>None</code> <code>l2</code> <code>float</code> <p>L2 regularization parameter, apply to all trainable weights and biases.</p> <code>0.0</code> <p>Example</p> <pre><code>import tensorflow as tf\nfrom connectionist.data import ToyOP\nfrom connectionist.models import PMSP\n\ndata = ToyOP()\nmodel = PMSP(tau=0.2, h_units=10, p_units=9, c_units=5)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n)\nmodel.fit(data.x_train, data.y_train, epochs=3, batch_size=20)\n</code></pre>"},{"location":"models/PMSP/#connectionist.models.PMSP.weights_abbreviations","title":"<code>weights_abbreviations: Dict[str, str]</code>  <code>property</code>","text":"<p>Weight abbreviation to internal name mapping.</p> <p>Note</p> <p>Technically the full internal name will change when creating more than one instance of the same model.     As a workaround, we use the last section of the internal name here as a matching string.</p>"},{"location":"models/PMSP/#connectionist.models.PMSP.add_noise","title":"<code>add_noise(layer, stddev)</code>","text":"<p>Add noise to the target layer.</p> <p>The noise is active in both training and inference.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>the target layer, choose from ['hidden', 'phonology', 'cleanup']</p> required <code>stddev</code> <code>float</code> <p>the standard deviation of the noise</p> required <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A new model with the same architecture, but with new noise.</p> <p>Example</p> <pre><code>from connectionist.models import PMSP\n\nmodel = PMSP(tau=0.2, h_units=10, p_units=9, c_units=5)\nmodel.build(input_shape=[1, 30, 10])\nnew_model = model.add_noise('hidden', stddev=0.1)\n</code></pre>"},{"location":"models/PMSP/#connectionist.models.PMSP.apply_l2","title":"<code>apply_l2(l2)</code>","text":"<p>Add L2 regularization to all the weights and biases in the model.</p> <p>Parameters:</p> Name Type Description Default <code>l2</code> <code>float</code> <p>the L2 regularization rate.</p> required <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A new model with the same architecture, but with L2 regularization applied to all the weights and biases.</p> <p>Example</p> <pre><code>from connectionist.models import PMSP\n\nmodel = PMSP(tau=0.2, h_units=10, p_units=9, c_units=5)\nmodel.build(input_shape=[1, 30, 10])\nnew_model = model.apply_l2(l2=0.1)\n</code></pre>"},{"location":"models/PMSP/#connectionist.models.PMSP.call","title":"<code>call(inputs, training=False, return_internals=False)</code>","text":"<p>Forward pass, identical to PMSPLayer.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>tf.Tensor</code> <p>Input tensor.</p> required <code>training</code> <code>bool</code> <p>Whether the model is in training mode.</p> <code>False</code> <code>return_internals</code> <code>bool</code> <p>Whether to return intermediate inputs to each connection.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>outputs</code> <code>Dict[str, tf.Tensor]</code> <p>a dictionary that stores all outputs.</p>"},{"location":"models/PMSP/#connectionist.models.PMSP.cut_connections","title":"<code>cut_connections(connections)</code>","text":"<p>Cut connections between two layers.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>List[str]</code> <p>the connections to be cut, the connections must be found in the original model, i.e., in <code>model.connections</code>.</p> required <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A new model with the same architecture, but with new connections.</p> <p>Example</p> <pre><code>from connectionist.models import PMSP\n\nmodel = PMSP(tau=0.2, h_units=10, p_units=9, c_units=5)\nmodel.build(input_shape=[1, 30, 10])\nnew_model = model.cut_connections(['pp', 'pc'])\n</code></pre>"},{"location":"models/PMSP/#connectionist.models.PMSP.shrink_layer","title":"<code>shrink_layer(layer, rate)</code>","text":"<p>Shrink the number of units in a layer, and all its dependent connections by random sampling.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>the target layer, choose from ['hidden', 'phonology', 'cleanup']</p> required <code>rate</code> <code>float</code> <p>the shrink rate</p> required <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A new model with the same architecture, but with new weights shapes that match with the shrank layer.</p> <p>Example</p> <pre><code>from connectionist.models import PMSP\n\nmodel = PMSP(tau=0.2, h_units=10, p_units=9, c_units=5)\nmodel.build(input_shape=[1, 30, 10])\nnew_model = model.shrink_layer('hidden', rate=0.5)\n</code></pre>"},{"location":"models/PMSP/#connectionist.models.PMSP.to_units","title":"<code>to_units(layer)</code>","text":"<p>Get the number of units in a target layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>the target layer, choose from ['hidden', 'phonology', 'cleanup']</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>the number of units in the target layer</p>"},{"location":"models/PMSP/#connectionist.models.PMSP.zero_out","title":"<code>zero_out(rates)</code>","text":"<p>Zero out weights of the target connections.</p> <p>Parameters:</p> Name Type Description Default <code>rates</code> <code>Dict[str, float]</code> <p>the zero out rates for each connection. e.g., {'hc': 0.5, 'ph': 0.4}. Higher zero out rates means more weights will be zeroed out.</p> required <p>Returns:</p> Type Description <code>tf.keras.Model</code> <p>A new model with the same architecture, but with new weights.</p> <p>Example</p> <pre><code>from connectionist.models import PMSP\n\nmodel = PMSP(tau=0.2, h_units=10, p_units=9, c_units=5)\nmodel.build(input_shape=[1, 30, 10])\nnew_model = model.zero_out(rates={'hp': 0.5, 'pc': 0.4})\n</code></pre>"},{"location":"models/overview/","title":"Connectionist.models","text":"<p>There are 2 models in this module:</p> <ul> <li>Orthography-to-Phonology model (PMSP) by Plaut, McClelland, Seidenberg and Patterson (1996), simulation 3.</li> <li>Hub-and-spokes model by Rogers et. al. (2004).</li> </ul>"},{"location":"models/overview/#brain-damaging-apis","title":"Brain damaging APIs","text":"<p>Currently only available in PMSP</p> <ul> <li>Reduce number of units in a layer</li> <li>Set some portion of weight in a connection to zero (and make it un-trainable)</li> <li>Remove the entire connection between two layers</li> <li>Introduce noise to a layer's input</li> <li>Put stress to keep the weights small (L2 regularization)</li> </ul>"},{"location":"surgery/Surgeon/","title":"Surgeon","text":"<p>A surgeon transplanting weights from one model to another according to a SurgeryPlan.</p> <p>The SurgeryPlan only specify where the shrinkage happens, when calling transplant, it will:</p> <ul> <li>use lesion_transplant to copy over the weights that requires shrinking.</li> <li>use copy_transplant on the remaining weights.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>surgery_plan</code> <code>SurgeryPlan</code> <p>A surgery plan for the transplant.</p> required <p>Example</p> <pre><code>import tensorflow as tf\nfrom connectionist.data import ToyOP\nfrom connectionist.models import PMSP\nfrom connectionist.surgery import SurgeryPlan, Surgeon, make_recipient\n\n# Create model and train\ndata = ToyOP()\nmodel = PMSP(tau=0.2, h_units=10, p_units=9, c_units=5)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n)\nmodel.fit(data.x_train, data.y_train, epochs=10, batch_size=20)\n\n# Create surgery plan and surgeon\nplan = SurgeryPlan(layer='hidden', original_units=10, shrink_rate=0.3, make_model_fn=PMSP)\nsurgeon = Surgeon(surgery_plan=plan)\n\n# Create recipient model and transplant weights\nnew_model = make_recipient(model=model, layer=plan.layer, keep_n=plan.keep_n, make_model_fn=plan.make_model_fn)\nnew_model.build(input_shape=model.pmsp._build_input_shape)\n\n# Transplant weights and biases\nsurgeon.transplant(donor=model, recipient=new_model)\n</code></pre> <p>Also see connectionist.models.PMSP.shrink_layer for high-level API.</p>"},{"location":"surgery/Surgeon/#connectionist.surgery.Surgeon.lesion_transplant","title":"<code>lesion_transplant(donor, recipient, weight_name, keep_idx, axes)</code>","text":"<p>Transplant weights from donor to recipient in the weights that requires shrinking.</p> <p>Parameters:</p> Name Type Description Default <code>donor</code> <code>tf.keras.Model</code> <p>The donor model.</p> required <code>recipient</code> <code>tf.keras.Model</code> <p>The recipient model.</p> required <code>weight_name</code> <code>str</code> <p>The name of the weights to transplant.</p> required <code>keep_idx</code> <code>List[int]</code> <p>The indices to keep.</p> required <code>axes</code> <code>List[int]</code> <p>The axes to slice the weights, usually contains only 1 axis, but 2 when it is a self-connecting weight.</p> required"},{"location":"surgery/Surgeon/#connectionist.surgery.Surgeon.transplant","title":"<code>transplant(donor, recipient)</code>","text":"<p>Transplant all weights from donor to recipient model.</p> <p>Parameters:</p> Name Type Description Default <code>donor</code> <code>tf.keras.Model</code> <p>The donor model.</p> required <code>recipient</code> <code>tf.keras.Model</code> <p>The recipient model.</p> required"},{"location":"surgery/SurgeryPlan/","title":"SurgeryPlan","text":"<p>A surgery plan for shrinking a layer, removing <code>shrink_rate</code> amount of the units in <code>layer</code>.</p> <p>Reducing the units in a layer will also reduce the number units in all connected weights and bias. The index of removal is random and shared across all weights and biases.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>str</code> <p>The layer name to shrink. e.g.(hidden, phonology, cleanup in PMSP).</p> required <code>original_units</code> <code>int</code> <p>The original number of units in the layer.</p> required <code>shrink_rate</code> <code>float</code> <p>The shrink rate, between 0 and 1.</p> required <code>make_model_fn</code> <code>Callable</code> <p>A function that make the original and new model.</p> required"},{"location":"surgery/SurgeryPlan/#connectionist.surgery.SurgeryPlan.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate plan and random sample the indices of unit to keep.</p>"},{"location":"surgery/copy_transplant/","title":"copy_transplant","text":"<p>Transplant all specified weights and biases from <code>donor</code> to <code>recipient</code>.</p> <p>All weights and biases shapes must match.</p> <p>Parameters:</p> Name Type Description Default <code>donor</code> <code>tf.keras.Model</code> <p>The donor model.</p> required <code>recipient</code> <code>tf.keras.Model</code> <p>The recipient model.</p> required <code>weight_name</code> <code>str</code> <p>The name of the weights to transplant, can be full internal name, partial internal name (will match with .endswith) or abbreviation if <code>model.weights_abbreviation</code> exists).</p> required"},{"location":"surgery/make_recipient/","title":"make_recipient","text":"<p>Make a recipient model according to donor and surgery plan for shrinking a layer.</p> <p>Parameters:</p> Name Type Description Default <code>donor</code> <code>tf.keras.Model</code> <p>The donor model.</p> required <code>layer</code> <code>str</code> <p>The layer name to shrink. e.g.(hidden, phonology, cleanup in PMSP).</p> required <code>keep_n</code> <code>int</code> <p>The number of units to keep.</p> required <code>make_model_fn</code> <code>Callable</code> <p>A function that make the original and new model.</p> required"}]}